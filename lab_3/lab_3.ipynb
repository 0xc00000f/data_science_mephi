{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа № 3\n",
    "### Минимальная оценка - 3 балла\n",
    "### Максимальная оценка - 5 балла\n",
    "Цель работы: реализация алгоритмов машинного обучения, основанных на решающих деревьях.\n",
    "\n",
    "Для успешной сдачи лабораторной работы Вам необходимо предоставить заполненый исходный ноутбук, продемострировать процесс выполнения работы и ответить на дополнительные вопросы преподавателя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный датасет посвящён определению типа опухоли (доброкачественная или злокачественная).\n",
    "- id - идентификатор пользователя\n",
    "- diagnosis - диагноз (M = malignant (злокачественная опухоль), B = benign (доброкачественная)) - является меткой класса (далее целевая переменная)\n",
    "- столбцы 3-33 - параметры опухоли."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (3 балла)\n",
    "\n",
    "### Задание 1.1\n",
    "Загрузите датасет $breast\\_cancer\\_wisconsin\\_data.csv$, выполните его предобработку, разделите на обучающую и тестовую выборки (в\n",
    "соотношении 7:3) и выделите целевую переменную.\n",
    "\n",
    "*Подсказка: некоторые неинформативные поля можно удалить.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "            ...             radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           ...                    25.38          17.33           184.60   \n",
       "1           ...                    24.99          23.41           158.80   \n",
       "2           ...                    23.57          25.53           152.50   \n",
       "3           ...                    14.91          26.50            98.87   \n",
       "4           ...                    22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"breast_cancer_wisconsin_data.csv\")\n",
    "\n",
    "# удаление неинформативных полей\n",
    "# https://stackoverflow.com/questions/43983622/remove-unnamed-columns-in-pandas-dataframe\n",
    "data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# TODO нормализовать\n",
    "\n",
    "y = data[\"diagnosis\"]\n",
    "X = data.drop(\"diagnosis\", axis=1)\n",
    "\n",
    "M, N = data.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.2\n",
    "Создайте свой экземпляр [решающего дерева](https://scikit-learn.org/0.20/modules/generated/sklearn.tree.DecisionTreeClassifier.html), обучите его (пользуясь методом [fit()](https://scikit-learn.org/0.20/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.fit)) и сохраните предсказания в переменную $predict\\_y$ (метод [predict()](https://scikit-learn.org/0.20/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B' 'M' 'M' 'B' 'B' 'M' 'M' 'M' 'M' 'B' 'B' 'M' 'B' 'M' 'B' 'M' 'B' 'B'\n",
      " 'B' 'M' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B'\n",
      " 'M' 'B' 'M' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'B'\n",
      " 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'M' 'M'\n",
      " 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'M' 'M' 'M' 'M' 'M' 'B' 'B'\n",
      " 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'M' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'M'\n",
      " 'M' 'B' 'M' 'B' 'B' 'M']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth=4, min_samples_leaf=5)\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "clf.feature_importances_\n",
    "clf.score(X=X_test, y=y_test)\n",
    "\n",
    "predict_y = clf.predict(X_test)\n",
    "print(predict_y)\n",
    "\n",
    "# z = [predict_y, predict_y]\n",
    "# cnt = Counter()\n",
    "# for i in z: \n",
    "#     for i "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.3\n",
    "Реализуйте любые 2 метрики оценки качества классификации и, используя их, сравните Ваши предсказания($predict\\_y$) с тестовой целевой переменной.\n",
    "Результат выведете в отдельной ячейке в формате 'metric_name : score'. Например: $$accuracy: 0.756$$\n",
    "\n",
    "Подсказка: про метрики качества классификации можно прочитать [тут](https://habr.com/ru/company/ods/blog/328372/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision --accuracy: 0.9583333333333334\n",
      "recall --accuracy: 0.971830985915493\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def precision(y, y_hat, positive):\n",
    "    y = y.values\n",
    "    count = defaultdict(lambda:0)\n",
    "    for i in range(len(y)):\n",
    "        if (y[i] == positive) & (y_hat[i] == positive):\n",
    "            count[\"TP\"] += 1\n",
    "        if (y[i] != positive) & (y_hat[i] == positive):\n",
    "            count[\"FP\"] += 1\n",
    "    return \"precision --accuracy: \" + str(count[\"TP\"] / (count[\"TP\"] + count[\"FP\"]))\n",
    "\n",
    "def recall(y, y_hat, positive):\n",
    "    y = y.values\n",
    "    count = defaultdict(lambda:0)\n",
    "    for i in range(len(y)):\n",
    "        if (y[i] == positive) & (y_hat[i] == positive):\n",
    "            count[\"TP\"] += 1\n",
    "        if (y[i] == positive) & (y_hat[i] != positive):\n",
    "            count[\"FN\"] += 1\n",
    "    return \"recall --accuracy: \" + str(count[\"TP\"] / (count[\"TP\"] + count[\"FN\"]))\n",
    "    \n",
    "print(precision(y_test, predict_y, 'B'))\n",
    "print(recall(y_test, predict_y, 'B'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.4\n",
    "Реализуйте функцию, которая из исходного датасета (после вашей предобрадотки) формирует новый, путём случайного выбора $m < M$ столбцов и $n < N$ строк (где $M$ и $N$ - число столбцов и строк в исходном датасете). Значения $m$ и $n$ являются аргументами функции. Результат работы функции продемострируйте на исходном датасете с $n=2$ и $m=2$.\n",
    "\n",
    "Например. Пусть исходный датасет имеет 4 столбца (**A,B,C,D**) и 5 строк. Тогда результат работы Вашей функции при $m=2$ (случайно выбрались 2 столбца: **A,C** ) и $n=3$ (случайно выбрались 3 строчки: 1,3,5) должен быть следующим:\n",
    "![](https://pp.userapi.com/c849424/v849424121/144e71/E-uOYJylgIs.jpg)\n",
    "\n",
    "Для многократного использования функции постарайтесь не удалять строки и столбцы из исходного датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>radius_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.07325</td>\n",
       "      <td>16.41</td>\n",
       "      <td>0.2116</td>\n",
       "      <td>13.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>0.05584</td>\n",
       "      <td>14.35</td>\n",
       "      <td>0.1619</td>\n",
       "      <td>13.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.05586</td>\n",
       "      <td>14.91</td>\n",
       "      <td>0.1589</td>\n",
       "      <td>14.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.05586</td>\n",
       "      <td>16.22</td>\n",
       "      <td>0.1635</td>\n",
       "      <td>14.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fractal_dimension_mean  radius_worst  symmetry_mean  radius_mean\n",
       "512                 0.07325         16.41         0.2116        13.40\n",
       "457                 0.05584         14.35         0.1619        13.21\n",
       "439                 0.05586         14.91         0.1589        14.02\n",
       "298                 0.05586         16.22         0.1635        14.26"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rand_func(dataset_A, m=4, n=4, random_state=0):\n",
    "    return dataset_A[dataset_A.columns.to_series().sample(n=m, random_state=random_state)].sample(n=n, random_state=random_state)\n",
    "    \n",
    "rand_func(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.5\n",
    "Реализуйте свой класс композиции решающих деревьев (ансамбля решающих деревьев) с параметрами:\n",
    "- base_model_class - класс базовой модели\n",
    "- n_base_models - количество решающих деревьев в ансамбле\n",
    "- n_features - аналог параметра $m$ в функции **rand_func** из задания 1.4\n",
    "- n_samples - аналог параметра $n$ в функции **rand_func** из задания 1.4\n",
    "\n",
    "Для реализации необходимо дописать методы $train$ и $predict$.\n",
    "\n",
    "В методе $train$ необходимо обучить все *self.n_base_models* базовых моделей, хранящихся в массиве *self.base_models*, на **разных** подвыборках размером $n х m$, где $m=\\sqrt{M}$, $n=\\sqrt{N}$ *(для получения разных подвыборок можно использовать функцию rand_func из задания 1.4 с параметрами $m=\\sqrt{M}$, $n=\\sqrt{N}$ и разными значениями $random\\_state$)*\n",
    "$$random\\_state=k*all*(i+1)$$где $k$- Ваш номер в списке группы, $all$-количество человек в Вашей группе, $i$-номер базовой модели в массиве *self.base_models*\n",
    "\n",
    "В методе $predict$ необходимо получить предсказания каждой базовой модели на тестовой выборке и объединить результаты с помощью простого голосования.\n",
    "\n",
    "Например:\n",
    "Если большинство решающих деревьев (predict_tree_ ) отнесло строку к классу 1, то ответ ансабля (predict_ensemble) должен быть равен 1.\n",
    "\n",
    "![](https://pp.userapi.com/c849424/v849424121/144f8c/aq28uXn5gSE.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_ensemble(object):\n",
    "    \n",
    "    def __init__(self, base_model_class, n_base_models, n_features=None, n_samples=None, **base_model_params):\n",
    "        self.base_model_class = base_model_class\n",
    "        self.n_base_models = n_base_models # число решающих деревьев\n",
    "        self.n_features = n_features # число столбцов в подвыборке (m)\n",
    "        self.n_samples=n_samples # число строк в подвыборке (n)\n",
    "        self.base_models = [] # массив, содержащий n_base_models решающих деревьев\n",
    "        self.features = []\n",
    "        for i in range(n_base_models):\n",
    "            self.base_models.append(base_model_class(**base_model_params)) \n",
    "        \n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train classifier by calling .train() method of base models\n",
    "        :param X: array-like features (n_obj x n_features)\n",
    "        :param y: array-like targets\n",
    "        \"\"\"\n",
    "        i=0\n",
    "        for tree in self.base_models:\n",
    "            rs = 4*4*(i+1) #𝑟𝑎𝑛𝑑𝑜𝑚_𝑠𝑡𝑎𝑡𝑒=𝑘∗𝑎𝑙𝑙∗(𝑖+1)\n",
    "            train_X = rand_func(X, m=self.n_features, n=self.n_samples, random_state=rs)\n",
    "            self.features.append(train_X.columns)\n",
    "            train_Y = y[train_X.index.values]\n",
    "            \n",
    "            tree.fit(X=train_X, y=train_Y)\n",
    "            i+=1\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make prediction for X using .predict() method of base models and simple voting\n",
    "        :param X: array-like features (n_obj x n_features)\n",
    "        :return results: array-like target predictions (n_obj)\n",
    "        \"\"\"\n",
    "        pred_list = []\n",
    "        for i in range(len(self.base_models)):\n",
    "            tree = self.base_models[i]\n",
    "            pred_list.append(tree.predict(X[self.features[i]]))\n",
    "        \n",
    "        import numpy as np\n",
    "        pred_np = np.vstack(pred_list).T\n",
    "        \n",
    "        print(pd.DataFrame(pred_np))\n",
    "        \n",
    "        predict_list = []       \n",
    "        for i in range(pred_np.shape[0]):\n",
    "            predict_list.append(np.unique(pred_np[i], return_counts=True)[0])\n",
    " \n",
    "        return predict_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продемонстрируйте работу ансамбля. В качестве базовой модели возьмите [DecisionTreeClassifier](https://scikit-learn.org/0.20/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "$n\\_base\\_models=k*all$, где $k$-ваш номер в списке группы, $all$ - количество человек в группе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15\n",
      "0    B  B  B  B  B  M  B  B  B  B  B  B  B  B  B  B\n",
      "1    M  M  M  M  M  M  M  M  M  M  M  M  M  M  B  M\n",
      "2    M  M  B  M  M  M  B  M  M  M  M  M  M  M  M  M\n",
      "3    B  B  B  B  B  B  B  B  B  B  B  B  B  B  M  B\n",
      "4    B  B  B  B  B  B  M  B  B  B  B  B  B  B  M  B\n",
      "5    M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M\n",
      "6    M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M\n",
      "7    M  M  M  M  M  M  M  M  M  M  B  M  M  M  M  M\n",
      "8    B  M  B  M  B  B  M  B  M  B  B  B  B  B  M  B\n",
      "9    B  B  B  B  B  B  B  B  B  B  B  M  B  B  B  B\n",
      "10   B  B  B  B  B  B  B  B  B  B  M  M  B  B  B  B\n",
      "11   M  M  M  M  M  M  M  M  M  M  B  M  M  M  B  M\n",
      "12   B  B  B  B  B  B  B  M  B  B  M  M  B  M  B  B\n",
      "13   B  B  B  B  M  M  B  M  M  M  M  M  M  M  M  M\n",
      "14   B  B  B  M  B  B  M  B  B  B  B  B  B  B  B  B\n",
      "15   M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M\n",
      "16   B  B  B  B  B  B  B  M  B  B  B  M  B  B  B  B\n",
      "17   B  B  B  B  B  B  B  B  B  B  M  B  B  B  B  B\n",
      "18   B  B  B  B  B  B  B  B  B  B  B  B  B  B  B  B\n",
      "19   M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M\n",
      "20   B  B  B  M  B  B  B  B  M  B  B  M  B  M  M  B\n",
      "21   B  B  B  B  B  B  B  B  B  B  B  B  B  B  B  B\n",
      "22   M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M\n",
      "23   B  B  B  B  B  B  B  B  B  B  M  B  B  B  B  B\n",
      "24   B  B  B  B  B  B  M  B  B  B  M  B  B  B  B  B\n",
      "25   B  B  B  B  B  M  B  B  M  B  B  M  B  B  B  B\n",
      "26   B  B  B  B  B  B  B  B  B  B  B  M  B  B  B  B\n",
      "27   B  B  B  B  B  B  M  B  B  B  M  B  B  B  B  B\n",
      "28   B  B  B  B  B  B  B  B  B  B  B  B  B  B  B  B\n",
      "29   M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M\n",
      "..  .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..\n",
      "84   M  B  M  M  M  M  B  M  M  M  M  M  M  M  B  M\n",
      "85   M  M  B  M  M  M  M  M  M  B  B  M  M  M  M  M\n",
      "86   B  M  B  M  B  B  M  B  M  B  M  B  B  B  M  B\n",
      "87   M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M\n",
      "88   B  B  B  M  B  B  M  B  B  B  M  B  B  B  B  B\n",
      "89   B  B  B  M  B  B  B  B  B  B  B  B  B  B  B  B\n",
      "90   B  B  B  B  B  B  M  B  B  B  B  B  B  B  B  B\n",
      "91   B  B  B  M  B  B  M  B  B  B  B  M  B  B  M  B\n",
      "92   B  B  B  B  M  M  B  M  M  B  B  M  B  M  M  M\n",
      "93   B  B  B  B  B  B  B  B  B  B  M  B  B  B  B  B\n",
      "94   B  B  B  B  B  B  M  B  B  B  M  B  B  B  B  B\n",
      "95   B  B  B  B  B  M  B  B  B  B  B  B  B  B  B  B\n",
      "96   M  M  M  M  M  M  M  M  M  M  M  M  M  M  B  M\n",
      "97   M  M  B  M  M  M  M  B  M  M  M  M  B  M  M  B\n",
      "98   B  B  B  B  B  B  B  B  B  B  B  B  B  B  B  B\n",
      "99   M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M\n",
      "100  M  B  B  M  M  M  M  M  M  M  M  M  B  M  B  M\n",
      "101  B  B  B  B  B  B  M  B  B  B  B  B  B  B  B  B\n",
      "102  M  M  M  M  M  M  M  M  M  M  B  M  M  M  M  M\n",
      "103  M  B  M  M  M  M  M  M  M  M  M  M  M  M  M  M\n",
      "104  B  B  B  B  B  M  B  B  B  B  M  B  B  B  B  B\n",
      "105  B  B  B  B  B  M  M  B  B  B  B  B  B  B  B  B\n",
      "106  B  B  B  B  B  B  B  B  B  B  M  B  B  B  B  B\n",
      "107  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M  M\n",
      "108  B  B  B  M  B  B  M  M  M  B  B  M  B  M  B  B\n",
      "109  B  B  B  B  B  B  B  M  B  B  B  M  B  M  B  B\n",
      "110  M  M  B  M  M  M  M  M  M  M  M  M  M  M  B  M\n",
      "111  B  B  B  B  B  B  B  B  B  B  B  B  B  B  B  B\n",
      "112  B  B  B  B  B  M  B  M  B  B  M  M  B  B  B  B\n",
      "113  M  M  M  M  M  M  B  M  M  M  M  M  M  M  M  M\n",
      "\n",
      "[114 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import math\n",
    "n_base_models = 4*4\n",
    "n_features=int(math.sqrt(N))\n",
    "n_samples=int(math.sqrt(M))\n",
    "kwargs = {\"max_depth\":4, \"min_samples_leaf\":5}\n",
    "ensemble=my_ensemble(base_model_class=DecisionTreeClassifier, n_base_models=n_base_models, n_features=n_features, n_samples=n_samples, **kwargs)\n",
    "\n",
    "ensemble.train(X_train, y_train)\n",
    "y_hat_ens = ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.6\n",
    "Используя реализованные в задание 1.3 метрики, сравните точность предсказания ансамбля решающих деревьев с предсказанием одного решающего дерева из задания 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision --accuracy: 0.9583333333333334\n",
      "recall --accuracy: 0.971830985915493\n"
     ]
    }
   ],
   "source": [
    "print(precision(y_test, y_hat_ens, 'B'))\n",
    "print(recall(y_test, y_hat_ens, 'B'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2 (2 балла)\n",
    "Пользуясь материалами лекции, реализуйте свой класс решающего дерева. Критерий ветвления выбрать самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "def entropy(attribute_data):\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy\n",
    "    :param attribute_data: data from a single feature/attribute\n",
    "    :return: a float representing the Shannon entropy\n",
    "    \"\"\"\n",
    "    _, val_freqs = np.unique(attribute_data, return_counts=True)\n",
    "    # probabilities for each unique attribute value\n",
    "    val_probs = val_freqs / len(attribute_data)\n",
    "    return -val_probs.dot(np.log(val_probs))\n",
    "\n",
    "\n",
    "def info_gain(attribute_data, labels):\n",
    "    \"\"\"\n",
    "    Calculate information gain\n",
    "    :param attribute_data: data from single attribute\n",
    "    :param labels:\n",
    "    :return: a float representing information gain\n",
    "    \"\"\"\n",
    "    attr_val_counts = get_count_dict(attribute_data)\n",
    "    total_count = len(labels)\n",
    "    EA = 0.0\n",
    "    for attr_val, attr_val_count in attr_val_counts.items():\n",
    "        EA += attr_val_count * entropy(labels[attribute_data == attr_val])\n",
    "\n",
    "# Issue #1: Take entropy/information on global labels not on attribute data\n",
    "    return entropy(labels) - EA / total_count\n",
    "\n",
    "\n",
    "def get_count_dict(data):\n",
    "    \"\"\"\n",
    "    Return the unique values and their frequencies as a dictionary\n",
    "    :param data: a 1-D numpy array\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_values, data_freqs = np.unique(data, return_counts=True)\n",
    "    return dict(zip(data_values, data_freqs))\n",
    "\n",
    "\n",
    "def hypothesis_test(attribute_data, labels, p_threshold=None, return_p_value=False):\n",
    "    \"\"\"\n",
    "    Perform a chi-square test on the values for an attribute and their corresponding labels\n",
    "    :param attribute_data:\n",
    "    :param labels:\n",
    "    :param p_threshold:\n",
    "    :param return_p_value:\n",
    "    :return: True/False for p value exceeding threshold and optionally the p value tested\n",
    "    \"\"\"\n",
    "    # Get label frequencies\n",
    "    label_counts = get_count_dict(labels)\n",
    "    # Get attribute value frequencies\n",
    "    attr_val_counts = get_count_dict(attribute_data)\n",
    "    # Calculate length of data (outside of loops below)\n",
    "    total_count = len(labels)\n",
    "\n",
    "    # k and m will be used for degrees of freedom in chi-squared call\n",
    "    # k unique classes\n",
    "    k = len(label_counts)\n",
    "    # m unique attribute values\n",
    "    m = len(attr_val_counts)\n",
    "\n",
    "    statistic = 0.0\n",
    "    for attr_val, attr_val_count in attr_val_counts.items():\n",
    "        attr_val_ratio = attr_val_count / total_count\n",
    "        # Get corresponding label frequencies within this attribute value\n",
    "        label_counts_attr_val = get_count_dict(labels[attribute_data == attr_val])\n",
    "        for label_attr_val, label_count_attr_val in label_counts_attr_val.items():\n",
    "            # Expected label count is the probability of the attribute value by the\n",
    "            # probability of the label within the attribute\n",
    "            exp_label_count_attr_val = attr_val_ratio * label_counts[label_attr_val]\n",
    "            # Calculate the Chi-square statistic\n",
    "            statistic += (label_count_attr_val - exp_label_count_attr_val)**2 / exp_label_count_attr_val\n",
    "\n",
    "    # Calculate the p value from the chi-square distribution CDF\n",
    "    p_value = 1 - st.chi2.cdf(statistic, df=(m-1)*(k-1))\n",
    "\n",
    "    if return_p_value:\n",
    "        return p_value < p_threshold, p_value\n",
    "    else:\n",
    "        return p_value < p_threshold\n",
    "\n",
    "\n",
    "# Main decision tree class. There'll be one instance of the class per node.\n",
    "class DecisionTree:\n",
    "    # Main prediction at this node\n",
    "    label = None\n",
    "    # Split attribute for the children\n",
    "    attribute = None\n",
    "    # Attribute value (where attribute has been set by parent)\n",
    "    attribute_value = None\n",
    "    # A list of child nodes (DecisionTree)\n",
    "    children = None\n",
    "    # p value for hypothesis testing\n",
    "    p_value = None\n",
    "    # Threshold to test p value against\n",
    "    p_threshold = None\n",
    "    # the parent node (DecisionTree)\n",
    "    parent = None\n",
    "    # level down the tree. 1 is top level\n",
    "    level = None\n",
    "    # max depth, for pruning\n",
    "    max_level = 10000000\n",
    "\n",
    "    def __init__(self, data, labels, attributes, fitness_func=info_gain, value=None, parent=None, p_threshold=1.0, max_level=None, old_level=0):\n",
    "        \"\"\"\n",
    "        Create a Decision tree node\n",
    "        :param data: Attribute values (example inputs)\n",
    "        :param labels: example outputs\n",
    "        :param attributes: Attribute column references\n",
    "        :param fitness_func: A function to test goodness of fit\n",
    "        :param value: Value of the parent's split attribute\n",
    "        :param parent:\n",
    "        :param p_threshold: threshold for hypothesis test\n",
    "        :param max_level: maximum tree depth\n",
    "        :param old_level: parent's level in the tree\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.level = old_level + 1\n",
    "        self.p_threshold = p_threshold\n",
    "\n",
    "        if max_level is not None:\n",
    "            self.max_level = max_level\n",
    "\n",
    "        if value is not None:\n",
    "            self.attribute_value = value\n",
    "\n",
    "        if parent is not None:\n",
    "            self.parent = parent\n",
    "\n",
    "        # If data or remaining attributes are empty or we've reached max depth then set the node label to the most\n",
    "        # common one and return\n",
    "        if data.size == 0 or not attributes or self.level == self.max_level:\n",
    "            try:\n",
    "                # self.label = st.mode(labels)[0][0][0]\n",
    "                self.label = st.mode(labels)[0][0]\n",
    "            except:\n",
    "                self.label = labels[len(labels) - 1]\n",
    "            return\n",
    "\n",
    "        # If labels are all the same, set label and return\n",
    "        if np.all(labels[:] == labels[0]):\n",
    "            self.label = labels[0]\n",
    "            return\n",
    "\n",
    "        # If corresponding attribute values are the same on every example just pick the last label and return\n",
    "        # Implemented as a loop so we can stop checking as soon as we find a mismatch\n",
    "        examples_all_same = True\n",
    "        for i in range(1, data.shape[0]):\n",
    "            for j in range(data.shape[1]):\n",
    "                if data[0, j] != data[i, j]:\n",
    "                    examples_all_same = False\n",
    "                    break\n",
    "            if not examples_all_same:\n",
    "                break\n",
    "        if examples_all_same:\n",
    "            # Choose the last label\n",
    "            self.label = labels[len(labels) - 1]\n",
    "            return\n",
    "\n",
    "        # Build the tree by splitting the data and adding child trees\n",
    "        self.build(data, labels, attributes, fitness_func)\n",
    "        return\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.children is None:\n",
    "            return \"x[{0}]={1}, y={2}\".format(self.parent.attribute, self.attribute_value, self.label)\n",
    "        else:\n",
    "            if self.parent is not None:\n",
    "                return \"x[{0}]={1}, p={2}\".format(self.parent.attribute, self.attribute_value, self.p_value)\n",
    "            else:\n",
    "                return \"p={0}\".format(self.p_value)\n",
    "\n",
    "    def build(self, data, labels, attributes, fitness_func):\n",
    "        \"\"\"\n",
    "        build a subtree\n",
    "        :param data:\n",
    "        :param labels:\n",
    "        :param attributes:\n",
    "        :param fitness_func:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.choose_best_attribute(data, labels, attributes, fitness_func)\n",
    "        best_attribute_column = attributes.index(self.attribute)\n",
    "        # Attribute data is the single column with attribute values for the best attribute\n",
    "        attribute_data = data[:, best_attribute_column]\n",
    "\n",
    "        # Prune if hypothesis test fails\n",
    "        no_prune, self.p_value = hypothesis_test(attribute_data, labels, return_p_value=True, p_threshold=self.p_threshold)\n",
    "\n",
    "        if not no_prune:\n",
    "            # The try-return is probably not required here and above\n",
    "            try:\n",
    "                self.label = st.mode(labels)[0][0]\n",
    "            except:\n",
    "                self.label = labels[len(labels) - 1]\n",
    "            return\n",
    "\n",
    "        # The child trees will be passed data for all attributes except the split attribute\n",
    "        child_attributes = attributes[:]\n",
    "        child_attributes.remove(self.attribute)\n",
    "\n",
    "        self.children = []\n",
    "        for val in np.unique(attribute_data):\n",
    "            # Create children for data where the split attribute == val for each unique value for the attribute\n",
    "            child_data = np.delete(data[attribute_data == val,:], best_attribute_column,1)\n",
    "            child_labels = labels[attribute_data == val]\n",
    "            self.children.append(DecisionTree(child_data, child_labels, child_attributes, value=val, parent=self,\n",
    "                                              old_level=self.level, max_level=self.max_level))\n",
    "\n",
    "    def choose_best_attribute(self, data, labels, attributes, fitness):\n",
    "        \"\"\"\n",
    "        Choose an attribute to split the children on\n",
    "        :param data: values for all attributes\n",
    "        :param labels: values for corresponding labels\n",
    "        :param attributes: attribute columns\n",
    "        :param fitness: the closeness of fit function\n",
    "        :return: empty ... self.attribute will be set by this function instead\n",
    "        \"\"\"\n",
    "        best_gain = float('-inf')\n",
    "        for attribute in attributes:\n",
    "            attribute_data = data[:, attributes.index(attribute)]\n",
    "            gain = fitness(attribute_data, labels)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                self.attribute = attribute\n",
    "        return\n",
    "\n",
    "    def classify(self, data):\n",
    "        \"\"\"\n",
    "        Make predictions for the rows passed in data\n",
    "        :param data: rows of attribute values\n",
    "        :return: a numpy array of labels\n",
    "        \"\"\"\n",
    "        if data.size == 0:\n",
    "            return\n",
    "\n",
    "        # If we're down to one record then convert it back to a 2-D array\n",
    "        if len(data.shape) == 1:\n",
    "            data = np.reshape(data, (1,len(data)))\n",
    "\n",
    "        if self.children is None:\n",
    "            # If we're at the bottom of the tree then return the labels for all records as the tree node label\n",
    "            labels = np.ones(len(data)) * self.label\n",
    "            return labels\n",
    "\n",
    "        labels = np.zeros(len(data))\n",
    "\n",
    "        for child in self.children:\n",
    "            # Get the array indexes where the split attibute value  = child attribute value\n",
    "            child_attr_val_idx = data[:,self.attribute] == child.attribute_value\n",
    "            # pass the array subsets to child trees for classification\n",
    "            labels[child_attr_val_idx] = child.classify(data[child_attr_val_idx])\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct = 71.12299465240642\n"
     ]
    }
   ],
   "source": [
    "def read_file(filename, sep=' '):\n",
    "\n",
    "    \"\"\"\n",
    "    Read data from a text file\n",
    "    :param filename:\n",
    "    :param sep: field separation character\n",
    "    :return: 2-D list\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as input_file:\n",
    "        data = [[int(n) for n in line.rstrip('\\n').split(sep)] for line in input_file]\n",
    "    return data\n",
    "\n",
    "\n",
    "def bootstrap_replicate(data, labels):\n",
    "\n",
    "    \"\"\"\n",
    "    Sample with replacement from data\n",
    "    :param data:\n",
    "    :param labels:\n",
    "    :return: data and corresponding labels for samples (same lengths as originals)\n",
    "    \"\"\"\n",
    "\n",
    "    len_data = len(data)\n",
    "    idx = [np.random.randint(1, len_data) for _ in range(len_data)]\n",
    "\n",
    "    return data[idx], labels[idx]\n",
    "\n",
    "\n",
    "def zero_one_loss(y, y_prime):\n",
    "\n",
    "    \"\"\"\n",
    "    The zero-one loss function.\n",
    "    :param y:\n",
    "    :param y_prime:\n",
    "    :return: 0 if y = y_prime, 1 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    return np.asarray(y != y_prime, dtype=int)\n",
    "\n",
    "\n",
    "train_data = np.array(read_file('data/spect_train.txt', sep=','))\n",
    "train_labels = train_data[:,0]\n",
    "train_data = train_data[:,1:]\n",
    "test_data = np.array(read_file('data/spect_test.txt', sep=','))\n",
    "test_labels = test_data[:,0]\n",
    "test_data = test_data[:,1:]\n",
    "\n",
    "\n",
    "# create a map of the attributes so we can retain the original column numbers as the tree splits the data\n",
    "attributes = list(range(len(train_data[0])))\n",
    "\n",
    "# Do an initial run with the full training dataset\n",
    "correct=[]\n",
    "p_max = 1.0\n",
    "level_max = 9\n",
    "tree = DecisionTree(train_data, train_labels, attributes, p_threshold=p_max, max_level=level_max)\n",
    "y = tree.classify(test_data)\n",
    "\n",
    "print(\"correct = {}\".format(sum(np.asarray(y == test_labels, dtype=int))/len(y)*100))\n",
    "\n",
    "# Do 10 runs of 25-round bootstrap training varying the depth of the trees from 1 to 10 levels\n",
    "n = 25\n",
    "num_depths = 10\n",
    "bias = np.zeros(num_depths)\n",
    "variance = np.zeros(num_depths)\n",
    "accuracy = np.zeros(num_depths)\n",
    "depths = np.arange(1,num_depths + 1)\n",
    "\n",
    "for depth in depths:\n",
    "    y = np.zeros((n, len(test_data)))\n",
    "    \n",
    "    # We are assuming that N(x) = 0, so there's no noise. This means y_star = y_t\n",
    "    y_star = t = test_labels\n",
    "    for i in range(n):\n",
    "        boot_data, boot_labels = bootstrap_replicate(train_data, train_labels)\n",
    "        tree = DecisionTree(boot_data, boot_labels, attributes, p_threshold=p_max, max_level=depth)\n",
    "        y[i] = tree.classify(test_data)\n",
    "        \n",
    "    # Under zero-one loss the main prediction is the mode (least squares: mean, absolute loss: median)\n",
    "    y_m = st.mode(y,0)[0][0]\n",
    "\n",
    "    # What's the overall test accuracy of our prediction: correct / (correct + incorrect)\n",
    "    accuracy[depth - 1] = sum(np.asarray(y_m == y_star, int)) / len(y_star)\n",
    "\n",
    "    # Bias: average zero-one loss between the optimal and main predictions\n",
    "    bias[depth - 1] = np.mean(zero_one_loss(y_star, y_m))\n",
    "\n",
    "    # Variance: average {across examples} of [(+1 if main = optimal, -1 otherwise) *\n",
    "    #           average {across test datasets} zero-one loss between individual predictions and main prediction\n",
    "\n",
    "    c2 = np.asarray(y_m == y_star, dtype=int) * 2 - 1           # 1 if y_m == y_star, -1 otherwise\n",
    "    loss_ym_y = np.array([zero_one_loss(y_m, y_i) for y_i in y])\n",
    "    variance[depth - 1] = np.mean(c2 * np.mean(loss_ym_y,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Bias, Variance and overall accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(depths, bias)\n",
    "plt.plot(depths, variance)\n",
    "plt.plot(depths, accuracy, ls='--')\n",
    "plt.legend([\"Bias\", \"Variance\", \"Accuracy\"])\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('Loss %')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
